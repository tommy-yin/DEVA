import cv2
import os
import glob
import json
import random
from tqdm import tqdm
from PIL import Image
import math
import numpy as np
import torch
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer
import re
import warnings
import argparse
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from scipy.ndimage import gaussian_filter1d
import datetime

warnings.filterwarnings("ignore", category=UserWarning)

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

####################################
# æ¨¡å‹ã€å›¾ç‰‡é¢„å¤„ç†ç­‰å‡½æ•°
####################################


def setup_model(model_id):
    model = AutoModel.from_pretrained(
        model_id, torch_dtype="auto", device_map="auto",
        trust_remote_code=True
    ).eval()

    processor = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    return model, processor


def build_transform(input_size):
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])
    return transform


def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_ar = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_ar)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio


def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=True):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1)
        for i in range(1, n + 1) for j in range(1, n + 1)
        if i * j <= max_num and i * j >= min_num
    )
    target_ratios = sorted(target_ratios, key=lambda x: x[0]*x[1])
    target_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = int(image_size * target_ratio[0])
    target_height = int(image_size * target_ratio[1])
    blocks = target_ratio[0] * target_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    grid_w = target_width // image_size
    for i in range(blocks):
        box = (
            (i % grid_w) * image_size,
            (i // grid_w) * image_size,
            ((i % grid_w) + 1) * image_size,
            ((i // grid_w) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images


def load_image(image_file, input_size=448, max_num=12):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size)
    images = dynamic_preprocess(image, image_size=input_size, max_num=max_num, use_thumbnail=True)  # æ˜¯å¦ç”¨ thumbnail (Falseæ•ˆæœå¥½)
    # å¯¹æ¯ä¸€ä¸ª tile è¿›è¡Œ transformï¼Œå¹¶å †å æˆä¸€ä¸ªå¼ é‡
    pixel_values = torch.stack([transform(img) for img in images])
    return pixel_values  # è¿”å›å½¢çŠ¶ä¸º (tiles, C, H, W)


####################################
# å·¥å…·å‡½æ•°
####################################

def print_gpu_memory_usage(step_name=""):
    """
    æ‰“å°å½“å‰GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        cached = torch.cuda.memory_reserved() / 1024**3     # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"{step_name} - GPU Memory: {allocated:.2f}GB/{total:.2f}GB allocated, {cached:.2f}GB cached")
    else:
        print(f"{step_name} - No CUDA available")


def get_image_paths(root_folder):
    sample_json = []
    sample_json.extend(glob.glob(os.path.join(root_folder, '*.json')))

    sample_frames = []
    for json_file in sample_json:
        with open(json_file, 'r') as f:
            data = json.load(f)
            frames = data['sample_frames']
            sample_frames.extend(frames)
    return sample_frames


def parse_result(answer):
    """
    ä»å›å¤ä¸­æå–å¼‚å¸¸åˆ†æ•°ï¼ŒèŒƒå›´ä¸º 0.1 åˆ° 0.9ã€‚
    ä¼˜å…ˆæ ¹æ®æ­£åˆ™åŒ¹é…ï¼ŒåŒ¹é…æ ¼å¼ä¾‹å¦‚: "Output: [0.3]" æˆ– "Output: 0.7" æˆ–å…¶ä»–åŒ…å« 0.1-0.9 çš„æ ¼å¼
    å¦‚æœæœªåŒ¹é…ï¼Œåˆ™è¿”å›é»˜è®¤å¼‚å¸¸åˆ†æ•° 0.1ã€‚
    """
    import re
    # é¦–å…ˆå°è¯•åŒ¹é… Output: æ ¼å¼
    match = re.search(r'Output:\s*\[?\s*(0\.[1-9])\s*\]?', answer)
    if match:
        return float(match.group(1))
    
    # å°è¯•åŒ¹é…ä»»ä½• 0.1 åˆ° 0.9 çš„æ•°å­—
    match = re.search(r'\b(0\.[1-9])\b', answer)
    if match:
        return float(match.group(1))
    
    # å°è¯•åŒ¹é…æè¿°æ€§çš„åˆ†æ•°
    lower_answer = answer.lower()
    if any(word in lower_answer for word in ['low', 'normal', 'minimal', 'unlikely']):
        return 0.1
    elif any(word in lower_answer for word in ['moderate', 'medium', 'possible']):
        return 0.5
    elif any(word in lower_answer for word in ['high', 'likely', 'suspicious', 'anomal']):
        return 0.8
    
    # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›é»˜è®¤å€¼
    return 0.1





####################################
# å…³é”®å‡½æ•°
####################################


def vqa_multi_round_inference(image_path, questions_list, model, processor, max_new_tokens=512):
    """
    å•å›¾å¤šè½®å¯¹è¯çš„VQAå‡½æ•°
    
    å‚æ•°ï¼š
        image_path: å›¾ç‰‡è·¯å¾„
        questions_list: é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªé—®é¢˜ä¸ºå­—ç¬¦ä¸²
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨å®ä¾‹ï¼ˆtokenizerï¼‰
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    
    è¿”å›ï¼š
        responses: æ¯è½®å¯¹è¯çš„å“åº”åˆ—è¡¨
        history: æœ€ç»ˆçš„å¯¹è¯å†å²
    """
    # é¢„å¤„ç†å›¾ç‰‡ï¼Œå¾—åˆ° pixel_values å¼ é‡ï¼Œå¹¶é€å…¥åˆ°æ¨¡å‹
    pixel_values = load_image(image_path).to(torch.bfloat16).to(model.device)
    
    generation_config = dict(max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, top_p=0.9, top_k=50)
    
    responses = []
    history = None
    
    try:
        for i, question in enumerate(questions_list):
            # ç¬¬ä¸€è½®å¯¹è¯éœ€è¦åŒ…å«å›¾ç‰‡æ ‡è¯†ç¬¦
            if i == 0:
                formatted_question = f'<image>\n{question}'
            else:
                formatted_question = question
            formatted_question = f'{formatted_question} Please answer with 2-3 sentences.'
            
            # è¿›è¡Œå¯¹è¯
            response, history = model.chat(
                processor, 
                pixel_values, 
                formatted_question, 
                generation_config, 
                history=history, 
                return_history=True
            )
            
            responses.append(response)
            
            # æ¯è½®å¯¹è¯åæ¸…ç†ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    finally:
        # ç¡®ä¿æ¸…ç†æ˜¾å­˜
        del pixel_values
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return responses, history


def process_video_frames(root_folder, model, processor, guiding_questions_dict, result_file_path):
    """
    Processes video frames from root_folder.
    
    For each frame, it extracts the video folder from the frame path,
    then uses questions from guiding_questions_dict for multi-round conversation.
    
    Now saves results after each image processing to prevent data loss.
    
    Parameters:
        result_file_path: Path to save/load the JSON results file
    
    Returns:
      results: dict mapping video_folder to frame scores and responses.
    """
    # å°è¯•åŠ è½½å·²æœ‰ç»“æœ
    results = {}
    if os.path.exists(result_file_path):
        try:
            with open(result_file_path, "r", encoding="utf-8") as f:
                results = json.load(f)
            print(f"âœ… åŠ è½½å·²æœ‰ç»“æœæ–‡ä»¶: {result_file_path}")
            print(f"ğŸ“Š å·²æœ‰ {len(results)} ä¸ªè§†é¢‘æ–‡ä»¶å¤¹çš„ç»“æœ")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹å¤„ç†")
            results = {}
    else:
        print(f"ğŸ“ åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶: {result_file_path}")
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(result_file_path), exist_ok=True)
    
    # è·å–æŒ‡å®šæ•°é‡çš„å¸§
    frames = get_image_paths(root_folder)
    print(f"è·å–åˆ° {len(frames)} ä¸ªå¸§è¿›è¡Œå¤„ç†")
    
    # ç»Ÿè®¡å·²å¤„ç†çš„å¸§æ•°
    processed_count = 0
    for video_folder, frames_data in results.items():
        for frame_key, frame_data in frames_data.items():
            if isinstance(frame_data, dict) and 'reason' in frame_data and 'anomaly_score' in frame_data:
                processed_count += 1
    
    print(f"ğŸ“ˆ å·²å¤„ç† {processed_count} ä¸ªå¸§ï¼Œå‰©ä½™ {len(frames) - processed_count} ä¸ªå¸§")
    
    # è¿½åŠ çš„å¼‚å¸¸åˆ†æ•°é—®é¢˜
    anomaly_score_question = (
        "Using your reasoning above, pick an anomaly score for the frame from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, "
        "If any anomalies or violations of security rules are detected, a relatively high score is assigned to reflect potential risks.\n"
        "where lower means normal and higher means abnormal. Format strictly as:\n"
        "â€¢ Answers: [your brief reasoning]\n"
        "â€¢ Output Anomaly Score: [one value from the list above].\n"
        " No extra comments."
    )

    frames_processed_this_run = 0
    
    for idx, img_path in enumerate(tqdm(frames, desc="Processing frames")):
        video_folder = os.path.dirname(img_path)
        frame_key = os.path.basename(img_path)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªå¸§
        if (video_folder in results and 
            frame_key in results[video_folder] and 
            isinstance(results[video_folder][frame_key], dict) and
            'reason' in results[video_folder][frame_key] and 
            'anomaly_score' in results[video_folder][frame_key]):
            print(f"â­ï¸  è·³è¿‡å·²å¤„ç†å¸§: {video_folder}/{frame_key}")
            continue
        
        # æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available() and frames_processed_this_run % 5 == 0:
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3     # GB
            print(f"Frame {idx+1}/{len(frames)} - GPU Memory: {allocated:.2f}GB allocated, {cached:.2f}GB cached")
        
        # Ensure this video folder has an entry.
        if video_folder not in results:
            results[video_folder] = {}
        
        # è·å–è¯¥è§†é¢‘æ–‡ä»¶å¤¹å¯¹åº”çš„é—®é¢˜
        questions_nested = guiding_questions_dict.get(video_folder, {}).get(frame_key, {}).get("questions", [])
        print(questions_nested)

        # å°†åµŒå¥—çš„é—®é¢˜åˆ—è¡¨æ‰å¹³åŒ–
        questions_list = []
        for group in questions_nested:
            if isinstance(group, list):
                questions_list.extend(group)
            else:
                questions_list.append(group)
        
        # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜
        if not questions_list:
            print(f"Warning: No questions found for {video_folder}/{frame_key}, using default questions.")
            questions_list = ["Please describe the image in detail."]
        
        # ç¡®ä¿é—®é¢˜åˆ—è¡¨è‡³å°‘æœ‰ä¸€ä¸ªé—®é¢˜
        # questions_list.append("Please describe the image in detail.")

        # è¿½åŠ å¼‚å¸¸åˆ†æ•°é—®é¢˜
        questions_list.append(anomaly_score_question)
        
        try:
            print(f"ğŸ”„ å¤„ç†å¸§: {video_folder}/{frame_key} ({idx+1}/{len(frames)})")
            
            # ä½¿ç”¨å¤šè½®å¯¹è¯å‡½æ•°
            responses, history = vqa_multi_round_inference(
                image_path=img_path,
                questions_list=questions_list,
                model=model,
                processor=processor,
                max_new_tokens=512
            )
            
            # ä¿å­˜å®Œæ•´çš„å¯¹è¯å†å²
            if frame_key not in results[video_folder]:
                results[video_folder][frame_key] = {}
            results[video_folder][frame_key]['reason'] = history
            
            # ä»æœ€åä¸€ä¸ªå›ç­”ä¸­è§£æå¼‚å¸¸åˆ†æ•°
            last_response = responses[-1] if responses else ""
            anomaly_score = parse_result(last_response)
            results[video_folder][frame_key]['anomaly_score'] = anomaly_score
            
            print(f"âœ… å®Œæˆå¸§å¤„ç†ï¼Œå¼‚å¸¸åˆ†æ•°: {anomaly_score}")
            
            # ç«‹å³ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            try:
                with open(result_file_path, "w", encoding="utf-8") as f:
                    json.dump(results, f, ensure_ascii=False, indent=4)
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file_path}")
            except Exception as save_error:
                print(f"âš ï¸  ä¿å­˜ç»“æœå¤±è´¥: {save_error}")
            
            # æ¸…ç†ä¸´æ—¶å˜é‡
            del responses, history, last_response
            frames_processed_this_run += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¸§ {img_path} æ—¶å‡ºé”™: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶è®¾ç½®é»˜è®¤å€¼
            if frame_key not in results[video_folder]:
                results[video_folder][frame_key] = {}
            results[video_folder][frame_key]['reason'] = f"Error processing frame: {str(e)}"
            results[video_folder][frame_key]['anomaly_score'] = 0.1
            
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ç»“æœ
            try:
                with open(result_file_path, "w", encoding="utf-8") as f:
                    json.dump(results, f, ensure_ascii=False, indent=4)
                print(f"ğŸ’¾ é”™è¯¯å¤„ç†ç»“æœå·²ä¿å­˜")
            except Exception as save_error:
                print(f"âš ï¸  ä¿å­˜é”™è¯¯ç»“æœå¤±è´¥: {save_error}")
        
        finally:
            # æ¯å¤„ç†å®Œä¸€å¼ å›¾ç‰‡åå¼ºåˆ¶æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¯å¤„ç†5å¼ å›¾ç‰‡åè¿›è¡Œåƒåœ¾å›æ”¶
            if frames_processed_this_run % 5 == 0:
                import gc
                gc.collect()

    print(f"ğŸ‰ æ€»å…±å¤„ç†äº† {len(frames)} ä¸ªå¸§ï¼Œæ¥è‡ª {len(results)} ä¸ªè§†é¢‘æ–‡ä»¶å¤¹")
    print(f"ğŸ“ˆ æœ¬æ¬¡è¿è¡Œæ–°å¤„ç†äº† {frames_processed_this_run} ä¸ªå¸§")
    return results


def main():
    args = parse_args()
    
    # æ¸…ç†æ˜¾å­˜ç¼“å­˜
    torch.cuda.empty_cache()
    print_gpu_memory_usage("Initial")
    
    model, processor = setup_model(args.model_id)
    print_gpu_memory_usage("After loading model")
    
    # ---------- Load CRAD_results.json for guiding questions ----------
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™éœ€è¦å…ˆè¿è¡ŒVAD_1å’ŒVAD_2
    if not os.path.exists(args.predict_result_dir):
        print(f"âš ï¸  ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.predict_result_dir}")
        print("è¯·ç¡®ä¿å·²è¿è¡ŒVAD_1_frame_caption.pyå’ŒVAD_2_route_plan.pyç”Ÿæˆåˆå§‹ç»“æœæ–‡ä»¶")
        return
    
    with open(args.predict_result_dir, "r", encoding="utf-8") as f:
        crad_results = json.load(f)
    
    print(f"âœ… åŠ è½½CRADç»“æœæ–‡ä»¶: {args.predict_result_dir}")
    print(f"ğŸ“Š åŒ…å« {len(crad_results)} ä¸ªè§†é¢‘æ–‡ä»¶å¤¹")
    
    # Pass the complete CRAD results to have access to questions for each video
    guiding_questions_dict = crad_results
    
    # ---------- Model Inference ----------
    # è¿™ä¸ªå‡½æ•°ç°åœ¨ä¼šè‡ªåŠ¨ä¿å­˜æ¯ä¸ªå¤„ç†å®Œçš„å›¾ç‰‡ç»“æœ
    final_results = process_video_frames(args.data_root, model, processor, guiding_questions_dict, args.predict_result_dir)
    
    print_gpu_memory_usage("After processing all frames")
    
    print(f"ğŸ‰ æ‰€æœ‰å¸§å¤„ç†å®Œæˆï¼ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {args.predict_result_dir}")
    
    # æœ€ç»ˆæ¸…ç†
    torch.cuda.empty_cache()
    print_gpu_memory_usage("Final cleanup")


def parse_args():
    parser = argparse.ArgumentParser(description="å¼‚å¸¸è§†é¢‘å¸§æ£€æµ‹ä¸åˆ†ç±» - InternVL3-8B")
    parser.add_argument("--data_root", type=str, default="/root/Ours_in_TAD/module_2/result_sample_TAD_Q15", help="é‡‡æ ·å¸§åˆ—è¡¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--predict_result_dir", type=str, 
                        default="/root/Ours_in_TAD/module_3/result_VAD_TAD_012_Q15/result_TAD_Q15.json", 
                        help="ä¿å­˜ç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_id", type=str, default="/root/autodl-tmp/InternVL3-8B", help="ä½¿ç”¨çš„æ¨¡å‹IDæˆ–è·¯å¾„")
    parser.add_argument('--is_validation', type=lambda x: x.lower() == "true", default=False, help='æ˜¯å¦ ground truth æ ¡éªŒ')
    
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    main()
